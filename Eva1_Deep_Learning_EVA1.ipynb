{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m6MaVY7LdaPn"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.datasets import fashion_mnist# descaraga la dataset\n","from tensorflow import keras\n","import numpy as np\n","import tensorflow as tf\n","from matplotlib import pyplot as plt"]},{"cell_type":"markdown","source":["Fashion-MNIST"],"metadata":{"id":"QmUpWCS3toH1"}},{"cell_type":"markdown","source":["Pedimos permisos para abir desde drive un dataset"],"metadata":{"id":"ad1ReOt9jVB4"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhyWoebql0lP","outputId":"ff2d8e62-1701-41c5-b24f-1db0f73d8374","executionInfo":{"status":"ok","timestamp":1712799114778,"user_tz":240,"elapsed":4509,"user":{"displayName":"PAMELA ROSA ALDANA ROJAS","userId":"08455162234145327811"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Ruta del dataset y almacenamiento de la info en listas"],"metadata":{"id":"NES4OiE6jdkn"}},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/My Drive/fashion-mnist-master\")\n"],"metadata":{"id":"WsSq9wKhx7eL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(imgs_train,labs_train), (imgs_test,labs_test) = keras.datasets.mnist.load_data()\n","print(imgs_train.shape)\n","print(imgs_test.shape)\n","\n"],"metadata":{"id":"FGWw5fljzY6b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pedimos las dimenciones de las imagenes y cantidad de datos de train y test"],"metadata":{"id":"CIiA3WS9jk-H"}},{"cell_type":"code","source":["(imgs_train,labs_train), (imgs_test,labs_test) = keras.datasets.mnist.load_data()\n","print(imgs_train.shape)\n","print(imgs_test.shape)\n","\n","# Tranformamos los datos para el entrenamiento,\n","# Necesitamos transformar tanto el train como el test\n","\n","X_train = imgs_train.reshape(60000,28*28)\n","X_test = imgs_test.reshape(10000,28*28)\n","X_train = X_train / 255\n","X_test = X_test / 255\n","print('X', X_train.shape, X_test.shape)\n","\n","Y_train = keras.utils.to_categorical(labs_train, 10)\n","Y_test = keras.utils.to_categorical(labs_test, 10)\n","print('Y', Y_train.shape, Y_test.shape)\n","\n","# cantidad de ejemplos (train/test), neuronas de entrada y neuronas de salida\n","\n","M_train = X_train.shape[0]\n","M_test = X_test.shape[0]\n","\n","N = X_train.shape[1]\n","C = Y_train.shape[1]\n","\n","print(N,C,M_train,M_test)\n","\n","# Setamos los seed para números random\n","\n","np.random.seed(30)\n","tf.random.set_seed(30)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xIcphCejGzUS","outputId":"7190604b-0e8e-4689-c88c-1c8393a77a78","executionInfo":{"status":"ok","timestamp":1712799123500,"user_tz":240,"elapsed":712,"user":{"displayName":"PAMELA ROSA ALDANA ROJAS","userId":"08455162234145327811"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n","(10000, 28, 28)\n"]}]},{"cell_type":"markdown","source":["Transformamos los datos para tener en la misma escala de magnitudes y determinamos la cantidad de pixel"],"metadata":{"id":"0ElosUHlj4Ih"}},{"cell_type":"code","source":["# Tranformamos los datos para el entrenamiento,\n","# Necesitamos transformar tanto el train como el test\n","\n","X_train = imgs_train.reshape(60000,28*28)\n","X_test = imgs_test.reshape(10000,28*28)\n","X_train = X_train / 255\n","X_test = X_test / 255\n","print('X', X_train.shape, X_test.shape)\n","\n","Y_train = keras.utils.to_categorical(labs_train, 10)\n","Y_test = keras.utils.to_categorical(labs_test, 10)\n","print('Y', Y_train.shape, Y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ArsUGzmg3Gm","outputId":"5955e0ac-e6f2-4891-895d-8f96586da8c7","executionInfo":{"status":"ok","timestamp":1712799231384,"user_tz":240,"elapsed":383,"user":{"displayName":"PAMELA ROSA ALDANA ROJAS","userId":"08455162234145327811"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X (60000, 784) (10000, 784)\n","Y (60000, 10) (10000, 10)\n"]}]},{"cell_type":"markdown","source":["asignamos variables que seran utilizadas en el modelamiento de la red neuronal"],"metadata":{"id":"42MJ_5AAkFYp"}},{"cell_type":"code","source":["# cantidad de ejemplos (train/test), neuronas de entrada y neuronas de salida\n","\n","M_train = X_train.shape[0]\n","M_test = X_test.shape[0]\n","\n","N = X_train.shape[1]\n","C = Y_train.shape[1]\n","\n","print(N,C,M_train,M_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tma7L-J2gE7H","outputId":"07d3c352-94fb-43a8-eebf-b9c3cf3c65ca","executionInfo":{"status":"ok","timestamp":1712799234268,"user_tz":240,"elapsed":358,"user":{"displayName":"PAMELA ROSA ALDANA ROJAS","userId":"08455162234145327811"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["784 10 60000 10000\n"]}]},{"cell_type":"code","source":["# Setamos los seed para números random\n","\n","np.random.seed(30)\n","tf.random.set_seed(30)\n"],"metadata":{"id":"QpdWkIQnLGiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cantidad de capas y neuronas"],"metadata":{"id":"AN5xrRQ2kPCy"}},{"cell_type":"code","source":["h1 = 512\n","h2 = 256"],"metadata":{"id":"s6EmCZLNjNz_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["red neuronal feed forward asignando funciones de activacion relu en las capas ocultas y obtenemos la arquitectura con red.sumary"],"metadata":{"id":"63tJhuXHkSD5"}},{"cell_type":"code","source":["# Creamos nuestra red feed-forward\n","\n","red = keras.Sequential()\n","red = keras.Sequential()\n","red.add(keras.layers.Dense(h1, input_dim=N, activation='relu', name='primera_capa'))\n","red.add(keras.layers.Dense(h2, activation='relu', name='segunda_capa'))\n","red.add(keras.layers.Dense(C, activation='softmax', name='capa_de_output'))\n","\n","red.summary()"],"metadata":{"id":"6nke-TmjG7gs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1bcd7619-9c03-4ab7-8ed8-f2bdcb297e0c","executionInfo":{"status":"ok","timestamp":1712799251789,"user_tz":240,"elapsed":732,"user":{"displayName":"PAMELA ROSA ALDANA ROJAS","userId":"08455162234145327811"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," primera_capa (Dense)        (None, 512)               401920    \n","                                                                 \n"," segunda_capa (Dense)        (None, 256)               131328    \n","                                                                 \n"," capa_de_output (Dense)      (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 535818 (2.04 MB)\n","Trainable params: 535818 (2.04 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Asignamos metricas y tambien optimizadores el cual sera sgd"],"metadata":{"id":"COXUxJTNkpS5"}},{"cell_type":"code","source":["red.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='sgd',\n","    metrics=['accuracy']\n",")"],"metadata":{"id":"24RCG7YkjJaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5ALSD6Ou8Mhi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["determinamos tamaño de los batch y cantidad de epocas"],"metadata":{"id":"2bfYlZq5k1_I"}},{"cell_type":"code","source":["hist = red.fit(X_train, Y_train,\n","        epochs=15,\n","        batch_size=128,\n","        validation_data=(X_test,Y_test)\n","       )"],"metadata":{"id":"msrdUioiLNsS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2ea59b7-cb20-4b2c-e9c3-647d41fce9d7","executionInfo":{"status":"ok","timestamp":1712799352416,"user_tz":240,"elapsed":91411,"user":{"displayName":"PAMELA ROSA ALDANA ROJAS","userId":"08455162234145327811"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","469/469 [==============================] - 8s 15ms/step - loss: 1.0589 - accuracy: 0.7592 - val_loss: 0.5058 - val_accuracy: 0.8802\n","Epoch 2/15\n","469/469 [==============================] - 7s 15ms/step - loss: 0.4370 - accuracy: 0.8863 - val_loss: 0.3617 - val_accuracy: 0.9020\n","Epoch 3/15\n","469/469 [==============================] - 7s 16ms/step - loss: 0.3515 - accuracy: 0.9022 - val_loss: 0.3137 - val_accuracy: 0.9141\n","Epoch 4/15\n","469/469 [==============================] - 5s 11ms/step - loss: 0.3124 - accuracy: 0.9117 - val_loss: 0.2876 - val_accuracy: 0.9200\n","Epoch 5/15\n","469/469 [==============================] - 6s 14ms/step - loss: 0.2870 - accuracy: 0.9190 - val_loss: 0.2664 - val_accuracy: 0.9268\n","Epoch 6/15\n","469/469 [==============================] - 5s 10ms/step - loss: 0.2678 - accuracy: 0.9246 - val_loss: 0.2517 - val_accuracy: 0.9298\n","Epoch 7/15\n","469/469 [==============================] - 6s 13ms/step - loss: 0.2523 - accuracy: 0.9288 - val_loss: 0.2381 - val_accuracy: 0.9323\n","Epoch 8/15\n","469/469 [==============================] - 5s 11ms/step - loss: 0.2387 - accuracy: 0.9332 - val_loss: 0.2285 - val_accuracy: 0.9331\n","Epoch 9/15\n","469/469 [==============================] - 5s 11ms/step - loss: 0.2267 - accuracy: 0.9363 - val_loss: 0.2175 - val_accuracy: 0.9371\n","Epoch 10/15\n","469/469 [==============================] - 7s 14ms/step - loss: 0.2160 - accuracy: 0.9393 - val_loss: 0.2075 - val_accuracy: 0.9408\n","Epoch 11/15\n","469/469 [==============================] - 5s 11ms/step - loss: 0.2060 - accuracy: 0.9428 - val_loss: 0.2006 - val_accuracy: 0.9409\n","Epoch 12/15\n","469/469 [==============================] - 7s 14ms/step - loss: 0.1971 - accuracy: 0.9446 - val_loss: 0.1911 - val_accuracy: 0.9444\n","Epoch 13/15\n","469/469 [==============================] - 5s 11ms/step - loss: 0.1887 - accuracy: 0.9474 - val_loss: 0.1848 - val_accuracy: 0.9457\n","Epoch 14/15\n","469/469 [==============================] - 5s 12ms/step - loss: 0.1810 - accuracy: 0.9490 - val_loss: 0.1780 - val_accuracy: 0.9488\n","Epoch 15/15\n","469/469 [==============================] - 6s 13ms/step - loss: 0.1739 - accuracy: 0.9508 - val_loss: 0.1720 - val_accuracy: 0.9503\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lzIWaIUIyuH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# imprime la pérdida para cada época\n","\n","plt.plot(h.history['loss'])\n","plt.title('Pérdida')\n","plt.xlabel('Época')\n","plt.show()"],"metadata":{"id":"vlqLS2X55pO-","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"1b72263d-a89b-4961-ca46-67756d043a04"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'h' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-b6e752449ca8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# imprime la pérdida para cada época\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pérdida'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Época'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"]}]},{"cell_type":"code","source":["# imprime el porcentaje de acierto para cada época\n","\n","plt.plot(h.history['accuracy'])\n","plt.title('% Acierto')\n","plt.xlabel('Época')\n","plt.show()"],"metadata":{"id":"N46mThvK5o_q","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"200f9d62-e1f9-4a58-a16d-a6710ccd1dc4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'h' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-6dbbb88efeb3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# imprime el porcentaje de acierto para cada época\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'% Acierto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Época'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"Sh8OSgEm0oew"}},{"cell_type":"markdown","source":["#RESUMEN\n"],"metadata":{"id":"n04WVX4G0Rkr"}},{"cell_type":"markdown","source":["Optimizador (optimizer): Este parámetro determina el algoritmo de optimización utilizado para ajustar los pesos del modelo durante el entrenamiento. Además de Adam, RMSprop y SGD,"],"metadata":{"id":"OC6AH4CtExaz"}},{"cell_type":"code","source":[],"metadata":{"id":"-fV_15-GOnlQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Función de pérdida (loss): define cómo se calculará la discrepancia entre las\n","predicciones del modelo y las etiquetas verdaderas durante el entrenamiento.\n"," <br> con etiquetas codificadas de forma one-hot, puedes usar sparse_categorical_crossentropy. Para problemas de clasificación binaria, puedes usar binary_crossentropy."],"metadata":{"id":"bycB_i_SE2xo"}},{"cell_type":"markdown","source":[" la función de pérdida sparse_categorical_crossentropy, que es adecuada para problemas de clasificación multiclase con etiquetas enteras."],"metadata":{"id":"ttQ_l1YUHjiW"}},{"cell_type":"markdown","source":["##generalizacion:\n","Para saber si un modelo está generalizando bien, es decir, si está aprendiendo patrones que se pueden aplicar a datos no vistos (conjunto de prueba), puedes seguir estos pasos:\n"," **Observa la precisión (accuracy) y la pérdida (loss) en este conjunto. Una alta precisión y una baja pérdida indican que el modelo está generalizando bien.**\n"," <br> **Compara la precisión y la pérdida en el conjunto de prueba con las métricas en el conjunto de entrenamiento. Si la precisión es similar en ambos conjuntos y la pérdida en el conjunto de prueba es baja, es probable que el modelo esté generalizando bien.**"],"metadata":{"id":"UYq5rvMbL9ON"}},{"cell_type":"markdown","source":["###generalizacion mala.\n","**Sobreajuste (Overfitting)**: rendimiento excelente en el conjunto de entrenamiento, pero su rendimiento en el conjunto de prueba es significativamente peor. Esto sugiere que el modelo ha aprendido demasiado los detalles específicos de los datos de entrenamiento y no puede generalizar correctamente a nuevos datos.<br>\n","**Subajuste (Underfitting)**: El modelo muestra un rendimiento deficiente tanto en el conjunto de entrenamiento como en el conjunto de prueba. Esto indica que el modelo es demasiado simple para capturar la complejidad de los datos y no puede aprender correctamente las relaciones entre las características de entrada y las etiquetas de salida.<br>\n","**Diferencia significativa entre pérdida de entrenamiento y pérdida de prueba:** Si la pérdida en el conjunto de entrenamiento es mucho más baja que la pérdida en el conjunto de prueba, podría indicar sobreajuste. Esto sugiere que el modelo está memorizando los datos de entrenamiento en lugar de aprender patrones generales."],"metadata":{"id":"SM1UitKfMvYB"}},{"cell_type":"markdown","source":["**Sobreajuste (Overfitting):**\n","Pérdida de validación creciente: Si la pérdida en el conjunto de validación comienza a aumentar después de un cierto número de épocas mientras que la pérdida en el conjunto de entrenamiento sigue disminuyendo, es una señal clara de sobreajuste. Esto indica que el modelo está memorizando los datos de entrenamiento en lugar de generalizar correctamente.\n","Precisión de validación decreciente: Similar a la pérdida de validación, si la precisión en el conjunto de validación comienza a disminuir mientras que la precisión en el conjunto de entrenamiento sigue aumentando, es una indicación de sobreajuste. Esto significa que el modelo está perdiendo su capacidad de generalización.\n","**Infraajuste (Underfitting):**\n","Pérdida alta en ambos conjuntos: Si la pérdida en ambos conjuntos (entrenamiento y validación) es alta y no disminuye significativamente a lo largo de las épocas, podría indicar infraajuste. Esto significa que el modelo es demasiado simple para capturar la estructura de los datos.\n","Precisión baja en ambos conjuntos: Similar a la pérdida, si la precisión en ambos conjuntos es baja y no mejora mucho durante el entrenamiento, podría ser un signo de infraajuste. Esto indica que el modelo no puede aprender de manera efectiva ni de los datos de entrenamiento ni de los datos de validación.<br>\n","Conclusiones:\n","Si ves que la pérdida y la precisión en el conjunto de entrenamiento son mucho mejores que en el conjunto de validación, es probable que el modelo esté sobreajustado.\n","Si ves que tanto la pérdida como la precisión son malas tanto en el conjunto de entrenamiento como en el conjunto de validación, es probable que el modelo esté infraajustado.\n","En resumen, debes observar cómo se comportan las métricas de pérdida y precisión en ambos conjuntos a lo largo del entrenamiento para determinar si tu modelo sufre de sobreajuste o infraajuste.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"x2JaVKH3Oatq"}},{"cell_type":"markdown","source":["#MUESTRA IMAGEN RANDOM Y SEGUN INDICE"],"metadata":{"id":"iYmpTFW70hsv"}},{"cell_type":"code","source":["# Seleccione una imagen aleatoria y su etiqueta correspondiente\n","index = np.random.randint(0, len(test_imagen))\n","image = test_imagen[index]\n","label = test_labels[index]\n","\n","# Grafica la imagen con su etiqueta\n","plt.figure()\n","plt.imshow(image, cmap='gray') # BLANCO Y NEGRO\n","plt.colorbar()\n","plt.title(f'Imagen: {clases[label]}')\n","plt.show()"],"metadata":{"id":"0rf9Bbhv0VZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i=1\n","img = train_imagen[i,:,i:]\n","plt.imshow(img, cmap='gray')\n","plt.show\n","print(f\"label: {clases[train_labels[i]]}\")\n"],"metadata":{"id":"vqLEXRpi0gAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mostramos una grilla de ejemplos (al azar)\n","\n","h = 4 # alto de la grilla\n","w = 4 # ancho de la grilla\n","fig, axs = plt.subplots(h, w, figsize=(2*h,2*w))\n","for i in range(h):\n","  for j in range(w):\n","    ex = np.random.randint(len(imgs_train))\n","    axs[i,j].set_title(\"etiqueta: \" + str(labs_train[ex]))\n","    axs[i,j].set_xticklabels([])\n","    axs[i,j].set_yticklabels([])\n","    axs[i,j].grid(False)\n","    axs[i,j].imshow(imgs_train\n","       [ex])"],"metadata":{"id":"JwrDZys_2BPO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para verificar que el set de datos esta en el formato adecuado y que estan listos para construir y entrenar la red, vamos a desplegar las primeras 25 imagenes de el training set y despleguemos el nombre de cada clase debajo de cada imagen"],"metadata":{"id":"qBXA064t2TAy"}},{"cell_type":"code","source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(train_imagen[i], cmap=plt.cm.binary)\n","    plt.xlabel(clases[train_labels[i]])\n","plt.show()"],"metadata":{"id":"HvgHJZcj2OlE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#MODELADO"],"metadata":{"id":"kuH3nSVWwfCr"}},{"cell_type":"code","source":["# Definir el modelo de un solo perceptron\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = Sequential([\n","    Flatten(input_shape=(28, 28)),              # Aplanar la imagen 28x28 a un vector de 784 elementos\n","    Dense(1, activation='sigmoid')])\n","\n","# Compilar el modelo\n","model.compile(optimizer='sgd',                              # Su función principal es ajustar los pesos del modelo durante el entrenamiento para minimizar la función de pérdida.\n","              loss='binary_crossentropy',                  # Utilizamos binary_crossentropy ya que es un problema de clasificación binaria\n","              metrics=['accuracy'])\n","                      # mide la exactud del modelo\n","# # Muestra un resumen de la arquitectura del modelo\n","model.summary()\n"," #Entrenamiento del modelo\n","\n","model.fit(train_imagen, train_labels, epochs=20, batch_size=32)\n","\n","#Evaluación del modelo\n","test_loss, test_acc = model.evaluate(test_imagen, test_labels)\n","print('Exactitud en los datos de prueba:', test_acc)\n","print('Perdida en los datos de prueba:', test_loss)"],"metadata":{"id":"TKpSLErkweCI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transformar las etiquetas a one-hot encoding\n"," #La codificación one-hot se usa comúnmente cuando se trabaja\n"," # con problemas de clasificación multiclase y se necesita representar\n"," #las etiquetas de manera binaria para el entrenamiento de modelos de aprendizaje automático.\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense\n","from tensorflow.keras.datasets import fashion_mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","train_labels = to_categorical(train_labels)\n","test_labels = to_categorical(test_labels)\n","\n"," #Cargar el conjunto de datos Fashion MNIST\n","(train_imagen, train_labels), (test_imagen, test_labels) = fashion_mnist.load_data()\n","\n","#Preprocesamiento de datos\n","train_imagen = train_imagen / 255.0\n","test_imagen = test_imagen / 255.0\n","\n","# Transformar las etiquetas a one-hot encoding\n","train_labels = to_categorical(train_labels)\n","test_labels = to_categorical(test_labels)\n","\n","# Definir el modelo de perceptrón multicapa\n","model = Sequential([\n","    Flatten(input_shape=(28, 28)),  # Capa de entrada: aplanar la imagen\n","    Dense(128, activation='relu'),  # Capa oculta: 128 neuronas con activación ReLU/ tanh\n","    Dense(10, activation='softmax')  # Capa de salida: 10 neuronas con activación Softmax, transforma las salidas crudas del modelo en una distribución de probabilidad sobre las clases\n","])\n","\n","# Compilar el modelo\n","model.compile(optimizer='adam',# RMSprop , SGD (Stochastic Gradient Descent)   #Adam puede ayudar más rápido y a evitar mínimos locales.\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","#optimización utilizado para ajustar los pesos del modelo durante el entrenamiento.\n","\n","# Muestra un resumen de la arquitectura del modelo\n","model.summary()\n","# Entrenar el modelo\n","h= model.fit(train_imagen, train_labels, epochs=20, batch_size=130, verbose=1)\n","\n","# Evaluar el modelo con los datos de prueba\n","test_loss, test_acc = model.evaluate(test_imagen, test_labels)\n","print(\"Precisión en el conjunto de prueba:\", test_acc)\n"],"metadata":{"id":"czHpDUYXHymm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CODIGO MANU"],"metadata":{"id":"ODsnBJI1xoVA"}},{"cell_type":"code","source":["#definimos las capas y las neuronas por capa oculta\n","\n","h1 = 512\n","\n","# Creamos nuestra red feed-forward\n","\n","\n","red = keras.Sequential()\n","red.add(keras.layers.Dense(h1, input_dim=N, activation='tanh', name='primera_capa_oculta'))\n","#red.add(keras.layers.Dense(h2, activation='relu', name='segunda_capa_oculta'))\n","#red.add(keras.layers.Dense(h2, activation='relu', name='tercera_capa_oculta'))\n","red.add(keras.layers.Dense(C, activation='softmax', name='capa_de_output'))\n","\n","# Imprimimos Arquitectura\n","red.summary()\n","\n","# Definimos metricas y optimizador\n","red.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='sgd',\n","    metrics=['accuracy']\n",")\n","# Definimos Batchs y Epocas\n","hist = red.fit(X_train, Y_train,\n","        epochs=20,\n","        batch_size=128,\n","        validation_data=(X_test,Y_test)\n","       )\n","\n","# imprime la pérdida para cada época\n","\n","plt.plot(hist.history['loss'])\n","plt.title('Pérdida')\n","plt.xlabel('Época')\n","plt.show()\n"],"metadata":{"id":"fAGGdRIHxc4Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelo 1 capa perceptron512\n","# Definir el modelo\n","model = Sequential([\n","    Flatten(input_shape=(28, 28)),  # Aplanar la imagen 28x28 a un vector de 784 elementos\n","    Dense(512, activation='relu'),   # Capa oculta con 512 perceptrones y activación ReLU\n","    Dense(10, activation='softmax')  # Capa de salida con 10 neuronas para las 10 clases y activación softmax\n","])\n","\n","# Compilar el modelo\n","model.compile(optimizer='adam',              # Optimizador Adam\n","              loss='sparse_categorical_crossentropy',  # Función de pérdida para clasificación multiclase\n","              metrics=['accuracy'])           # Métrica de precisión\n","\n","# Mostrar resumen del modelo\n","model.summary()\n","\n","# Entrenar el modelo\n","history = model.fit(train_imagen, train_labels, epochs=20, batch_size=32, validation_split=0.1)\n","\n","# Evaluar el modelo\n","test_loss, test_acc = model.evaluate(test_imagen, test_labels)\n","print('Precisión en los datos de prueba:', test_acc)\n","print('Pérdida en los datos de prueba:', test_loss)\n","\n","# Graficar la pérdida durante el entrenamiento\n","plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n","plt.plot(history.history['val_loss'], label='Pérdida de validación')\n","plt.title('Pérdida durante el entrenamiento')\n","plt.xlabel('Época')\n","plt.ylabel('Pérdida')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"RPVrRUU7S5Bv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Veamos algunas predicciones\n","h = 4\n","w = 4\n","fig, axs = plt.subplots(h, w, figsize=(4*h,4*w))\n","for i in range(h):\n","    for j in range(w):\n","        ex = np.random.randint(len(test_imagen))\n","        Xin = test_imagen[ex].reshape(1,28,28)\n","        Ypred = model.predict(Xin).reshape(10)\n","        lpred = np.argmax(Ypred)\n","\n","        axs[i,j].set_title(\"Imagen:\" + str(ex) + \"\\n real: \" + str(np.argmax(test_labels[ex])) + \" red: \" + str(lpred))\n","        axs[i,j].set_xticklabels([])\n","        axs[i,j].set_yticklabels([])\n","        axs[i,j].grid(False)\n","        axs[i,j].imshow(test_imagen[ex], cmap='gray')"],"metadata":{"id":"jQ_hpiW1TGPk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["codigo largo con 3 f activacion y una capa"],"metadata":{"id":"gq2BtxxD9jj5"}},{"cell_type":"code","source":["\n","# Cargar Fashion-MNIST dataset\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n","\n","# Normalizar los datos\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","\n","# Definir la cantidad de neuronas en la capa oculta\n","num_neurons = 512\n","\n","# Función de activación: relu\n","print(\"Probando con función de activación 'relu'\")\n","model_relu = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28)),\n","    tf.keras.layers.Dense(num_neurons, activation='relu'),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Compilar y entrenar modelo con función de activación relu\n","model_relu.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","\n","# Muestra un resumen de la arquitectura del modelo\n","model_relu.summary()\n","\n","history_relu = model_relu.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=0)\n","\n","# Evaluación del modelo con función de activación relu\n","test_loss_relu, test_acc_relu = model_relu.evaluate(X_test, y_test)\n","print(\"Accuracy del modelo fully connected con función de activación 'relu':\", test_acc_relu)\n","\n","# Graficar curvas de aprendizaje con función de activación relu\n","plt.plot(history_relu.history['accuracy'], label='accuracy')\n","plt.plot(history_relu.history['val_accuracy'], label = 'val_accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right')\n","plt.title(\"Función de activación: relu, Capa oculta: 1, Neuronas: 128\")\n","plt.show()\n","\n","# Función de activación: sigmoid\n","print(\"Probando con función de activación 'sigmoid'\")\n","model_sigmoid = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28)),\n","    tf.keras.layers.Dense(num_neurons, activation='sigmoid'),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Compilar y entrenar modelo con función de activación sigmoid\n","model_sigmoid.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","\n","# Muestra un resumen de la arquitectura del modelo\n","model_sigmoid.summary()\n","\n","history_sigmoid = model_sigmoid.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=0)\n","\n","# Evaluación del modelo con función de activación sigmoid\n","test_loss_sigmoid, test_acc_sigmoid = model_sigmoid.evaluate(X_test, y_test)\n","print(\"Accuracy funcion de activación 'sigmoid':\", test_acc_sigmoid)\n","\n","# Graficar curvas de aprendizaje con función de activación sigmoid\n","plt.plot(history_sigmoid.history['accuracy'], label='accuracy')\n","plt.plot(history_sigmoid.history['val_accuracy'], label = 'val_accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right')\n","plt.title(\"Función de activación: sigmoid, Capa oculta: 1, Neuronas: 512\")\n","plt.show()\n","\n","# Función de activación: tanh\n","print(\"Probando con función de activación 'tanh'\")\n","model_tanh = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28)),\n","    tf.keras.layers.Dense(num_neurons, activation='tanh'),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Compilar y entrenar modelo con función de activación tanh\n","model_tanh.compile(optimizer='adam',\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","# Muestra un resumen de la arquitectura del modelo\n","model_tanh.summary()\n","\n","history_tanh = model_tanh.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=0)\n","\n","# Evaluación del modelo con función de activación tanh\n","test_loss_tanh, test_acc_tanh = model_tanh.evaluate(X_test, y_test)\n","print(\"Accuracy de la función de activación 'tanh':\", test_acc_tanh)\n","\n","# Graficar curvas de aprendizaje con función de activación tanh\n","plt.plot(history_tanh.history['accuracy'], label='accuracy')\n","plt.plot(history_tanh.history['val_accuracy'], label = 'val_accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right')\n","plt.title(\"Función de activación: tanh, Capa oculta: 1, Neuronas: 512\")\n","plt.show()\n"],"metadata":{"id":"s1NqnpDo9g1Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#prediccion\n","h = 4\n","w = 4\n","fig, axs = plt.subplots(h, w, figsize=(4*h,4*w))\n","for i in range(h):\n","    for j in range(w):\n","        ex = np.random.randint(len(test_imagen))\n","        Xin = test_imagen[ex].reshape(1,28,28)\n","        Ypred = model.predict(Xin).reshape(10)\n","        lpred = np.argmax(Ypred)\n","\n","        axs[i,j].set_title(\"Imagen:\" + str(ex) + \"\\n real: \" + str(test_labels[ex]) + \" red: \" + str(lpred))\n","        axs[i,j].set_xticklabels([])\n","        axs[i,j].set_yticklabels([])\n","        axs[i,j].grid(False)\n","        axs[i,j].imshow(test_imagen[ex], cmap='gray')"],"metadata":{"id":"JQXiRTmuXe-f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###codigo completo guia manuel"],"metadata":{"id":"k_EvaleUdvj3"}},{"cell_type":"code","source":["# Cargar el conjunto de datos Fashion-MNIST\n","(imgs_train,labs_train), (imgs_test,labs_test) = keras.datasets.fashion_mnist.load_data()\n","# Imprimir las formas de los conjuntos de datos\n","print(imgs_train.shape)\n","print(imgs_test.shape)\n","\n","# Necesitamos transformar tanto el train como el test\n","# Transformar los datos para el entrenamiento\n","# Redimensionar los datos a una matriz de tamaño (60000, 784) para el conjunto de entrenamiento\n","# y (10000, 784) para el conjunto de prueba\n","\n","X_train = imgs_train.reshape(60000,28*28)\n","X_test = imgs_test.reshape(10000,28*28)\n","\n","# Normalizar los valores de píxeles dividiendo por 255 para escalarlos al rango [0, 1]\n","X_train = X_train / 255\n","X_train = X_train / 255\n","X_test = X_test / 255\n","print('X', X_train.shape, X_test.shape)\n","\n","# Convertir las etiquetas de clase en codificación one-hot\n","Y_train = keras.utils.to_categorical(labs_train, 10)\n","Y_test = keras.utils.to_categorical(labs_test, 10)\n","print('Y', Y_train.shape, Y_test.shape)\n","\n","# cantidad de ejemplos (train/test), neuronas de entrada y neuronas de salida\n","\n","M_train = X_train.shape[0]\n","M_test = X_test.shape[0]\n","\n","N = X_train.shape[1]\n","C = Y_train.shape[1]\n","\n","print(N,C,M_train,M_test)\n","\n","# Setamos los seed para números random\n","\n","np.random.seed(30)\n","tf.random.set_seed(30)"],"metadata":{"id":"h9lA4uIkdgR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tranformamos los datos para el entrenamiento,\n","# Necesitamos transformar tanto el train como el test\n","\n","# Transformamos las imágenes de entrenamiento de una forma (60000, 28, 28) a (60000, 784)\n","# para que cada imagen sea un vector unidimensional de tamaño 784 (28x28)\n","X_train = imgs_train.reshape(60000,28*28)\n","# Transformamos las imágenes de prueba de una forma (10000, 28, 28) a (10000, 784) de la misma manera\n","X_test = imgs_test.reshape(10000,28*28)\n","\n","#normalizan los valores de pixeles\n","# Esto escala los valores de píxeles al rango [0, 1], lo que puede ayudar a mejorar la convergencia durante el entrenamiento\n","X_train = X_train / 255\n","X_test = X_test / 255\n","# Imprimimos las formas de los conjuntos de datos transformados\n","print('X', X_train.shape, X_test.shape)\n","\n","# Convertimos las etiquetas de clase en codificación one-hot\n","# Por ejemplo, la etiqueta 2 se convierte en [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","Y_train = keras.utils.to_categorical(labs_train, 10)\n","Y_train = keras.utils.to_categorical(labs_train, 10)\n","Y_test = keras.utils.to_categorical(labs_test, 10)\n","# Imprimimos las formas de las etiquetas codificadas one-hot\n","\n","print('Y', Y_train.shape, Y_test.shape)"],"metadata":{"id":"moPz83_TdkSk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cantidad de ejemplos (train/test), neuronas de entrada y neuronas de salida\n","\n","# Calculamos la cantidad de ejemplos en el conjunto de entrenamiento y prueba\n","M_train = X_train.shape[0] # Número de ejemplos en el conjunto de entrenamiento\n","\n","M_test = X_test.shape[0]# Número de ejemplos en el conjunto de prueba\n","\n","# Calculamos el número de características de entrada (neuronas de entrada) y el número de clases (neuronas de salida)\n","N = X_train.shape[1]# Número de características en cada ejemplo de entrada (dimensiones de la imagen después de aplanar)\n","C = Y_train.shape[1]# Número de clases en la tarea de clasificación (10 clases en Fashion-MNIST)\n","\n","# Imprimimos el número de características de entrada, el número de clases, la cantidad de ejemplos en el conjunto de entrenamiento y la cantidad de ejemplos en el conjunto de prueba\n","print(N,C,M_train,M_test)"],"metadata":{"id":"KZhIpJRxdqim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setamos los seed para números random\n","\n","np.random.seed(30)\n","tf.random.set_seed(30)\n"],"metadata":{"id":"UkPqxcCXdtY1"},"execution_count":null,"outputs":[]}]}